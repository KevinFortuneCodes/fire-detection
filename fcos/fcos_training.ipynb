{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WEiAG-dNOEF2"
   },
   "source": [
    "# FCOS Fire/Smoke Detection Training\n",
    "\n",
    "This notebook provides a complete training and visualization environment for the FCOS fire/smoke detection model. **Works both locally and on Google Colab!**\n",
    "\n",
    "## Overview\n",
    "- Automatic Colab setup (mounts Drive, installs dependencies)\n",
    "- Load and inspect the detection dataset\n",
    "- **Train the FCOS model** (all epochs at once)\n",
    "- Visualize training progress\n",
    "- Run inference and visualize predictions\n",
    "- Evaluate model performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15929,
     "status": "ok",
     "timestamp": 1765139449726,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "CFCkrBY0OEF6",
    "outputId": "c92f21c8-dde7-43a9-f158-c43bfc966112"
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SETUP: Detect Colab and configure environment\n",
    "# ============================================================================\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"✓ Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"✓ Running locally\")\n",
    "\n",
    "# Colab-specific setup\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    import subprocess\n",
    "\n",
    "    # Mount Google Drive\n",
    "    print(\"Mounting Google Drive...\")\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "    # ⚠️ UPDATE THIS PATH to match your Google Drive folder\n",
    "    PROJECT_PATH = '/content/drive/MyDrive/fire-detection'\n",
    "    os.chdir(PROJECT_PATH)\n",
    "\n",
    "    # Setup paths first (needed for requirements.txt path)\n",
    "    project_root = Path(PROJECT_PATH)\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    sys.path.insert(0, str(project_root / 'fcos'))\n",
    "\n",
    "    # Install dependencies\n",
    "    print(\"Installing dependencies...\")\n",
    "    # Install PyTorch with CUDA support for Colab (must be first)\n",
    "    subprocess.run(['pip', 'install', 'torch', 'torchvision', 'torchaudio',\n",
    "                    '--index-url', 'https://download.pytorch.org/whl/cu118'],\n",
    "                   check=False)\n",
    "    # Install other dependencies from requirements.txt\n",
    "    # Note: PyTorch packages are already installed above, so pip will skip them\n",
    "    requirements_path = project_root / 'requirements.txt'\n",
    "    if requirements_path.exists():\n",
    "        subprocess.run(['pip', 'install', '-r', str(requirements_path)], check=False)\n",
    "        print(\"✓ Installed dependencies from requirements.txt\")\n",
    "    else:\n",
    "        # Fallback if requirements.txt not found\n",
    "        subprocess.run(['pip', 'install', 'pillow', 'matplotlib', 'numpy', 'tensorboard'],\n",
    "                       check=False)\n",
    "        print(\"⚠️  requirements.txt not found, installed basic dependencies\")\n",
    "\n",
    "    print(f\"✓ Colab setup complete!\")\n",
    "    print(f\"  Project root: {project_root}\")\n",
    "else:\n",
    "    # Local setup\n",
    "    project_root = Path().resolve().parent\n",
    "    sys.path.insert(0, str(project_root))\n",
    "    sys.path.insert(0, str(project_root / 'fcos'))\n",
    "    print(f\"✓ Local setup complete!\")\n",
    "    print(f\"  Project root: {project_root}\")\n",
    "\n",
    "# ============================================================================\n",
    "# IMPORTS\n",
    "# ============================================================================\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import dataset and FCOS model\n",
    "from fcos_dataset import FireSmokeDetectionDataset, collate_fn, create_dataloaders\n",
    "from fcos import FCOS\n",
    "\n",
    "# Import mapping functions from parent directory\n",
    "from phase2_detection_metadata import metadata_to_fcos_class, fcos_to_metadata_class, LABEL_MAP\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"✓ GPU available: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"⚠️  No GPU available - training will be slow\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLj-LkyIOEF8"
   },
   "source": [
    "## 0. Generate Metadata (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1765139449738,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "t_6EuRtcOEF9",
    "outputId": "79745030-cbc1-41a4-9b9d-7a674e57d0ce"
   },
   "outputs": [],
   "source": [
    "# Check if metadata JSON exists, generate if missing\n",
    "metadata_path = project_root / \"dfire_detection_annotations.json\"\n",
    "dataset_dir = project_root / \"D-Fire\"\n",
    "\n",
    "if metadata_path.exists():\n",
    "    print(f\"✓ Metadata file already exists: {metadata_path}\")\n",
    "    print(\"Skipping metadata generation.\")\n",
    "else:\n",
    "    print(\"Metadata file not found. Generating it now...\")\n",
    "\n",
    "    # Check if dataset exists\n",
    "    if not dataset_dir.exists():\n",
    "        print(f\"❌ Error: Dataset directory not found: {dataset_dir}\")\n",
    "        print(\"Please make sure the D-Fire dataset is in the project root.\")\n",
    "    else:\n",
    "        # Check if phase2_detection_metadata.py exists\n",
    "        metadata_script = project_root / \"phase2_detection_metadata.py\"\n",
    "        RESAMPLE_PERCENT = 30\n",
    "        if not metadata_script.exists():\n",
    "            print(f\"❌ Error: phase2_detection_metadata.py not found: {metadata_script}\")\n",
    "            print(\"Please make sure phase2_detection_metadata.py is in the project root.\")\n",
    "        else:\n",
    "            # Generate metadata\n",
    "            import subprocess\n",
    "            print(f\"Running: python phase2_detection_metadata.py ...\")\n",
    "            result = subprocess.run([\n",
    "                'python', str(metadata_script),\n",
    "                '--dataset-dir', str(dataset_dir),\n",
    "                '--output', str(metadata_path),\n",
    "                '--splits', 'train', 'test', 'validation',\n",
    "                '--resample-percent', str(RESAMPLE_PERCENT)\n",
    "            ], cwd=str(project_root), capture_output=True, text=True)\n",
    "\n",
    "            if result.returncode == 0:\n",
    "                if metadata_path.exists():\n",
    "                    print(f\"✓ Metadata generated successfully: {metadata_path}\")\n",
    "                else:\n",
    "                    print(\"⚠️  Script ran but metadata file not found. Check errors above.\")\n",
    "            else:\n",
    "                print(f\"❌ Error generating metadata:\")\n",
    "                print(result.stderr)\n",
    "                print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDPcwOHMOEF9"
   },
   "source": [
    "## 1. Dataset Inspection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1765139449801,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "e_JO9hNzOEF9",
    "outputId": "3bd227fc-c83c-4d25-bbe7-0ef8a699d8cb"
   },
   "outputs": [],
   "source": [
    "# Load metadata (automatically handles both local and Colab paths)\n",
    "# Note: metadata_path should already be defined from the previous cell\n",
    "if 'metadata_path' not in globals():\n",
    "    metadata_path = project_root / \"dfire_detection_annotations.json\"\n",
    "\n",
    "if not metadata_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Metadata file not found: {metadata_path}\\n\"\n",
    "        \"Please run the previous cell to generate it, or run phase2_detection_metadata.py manually.\"\n",
    "    )\n",
    "\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(\"Dataset splits:\", list(metadata[\"splits\"].keys()))\n",
    "\n",
    "for split_name, split_data in metadata[\"splits\"].items():\n",
    "    print(f\"\\n{split_name.upper()}:\")\n",
    "    print(f\"  Images: {split_data['num_images']}\")\n",
    "    print(f\"  Label distribution: {split_data['label_counts']}\")\n",
    "\n",
    "    # Count boxes by class\n",
    "    fire_boxes = 0\n",
    "    smoke_boxes = 0\n",
    "    for entry in split_data[\"entries\"]:\n",
    "        for ann in entry[\"annotations\"]:\n",
    "            if ann[\"class_idx\"] == 1:  # fire\n",
    "                fire_boxes += 1\n",
    "            elif ann[\"class_idx\"] == 2:  # smoke\n",
    "                smoke_boxes += 1\n",
    "    print(f\"  Total boxes - Fire: {fire_boxes}, Smoke: {smoke_boxes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "executionInfo": {
     "elapsed": 1644,
     "status": "ok",
     "timestamp": 1765139451504,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "p0CNlNFQOEF-",
    "outputId": "9497251e-745b-4799-c08c-71a5586cee2d"
   },
   "outputs": [],
   "source": [
    "# Visualize sample images with bounding boxes\n",
    "def visualize_sample(entry, ax=None):\n",
    "    \"\"\"Visualize a single image with its bounding boxes.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "    image_path = Path(entry[\"image_path\"])\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.set_title(f\"{entry['image_id']} - {entry['image_label_name']}\")\n",
    "\n",
    "    # Draw bounding boxes\n",
    "    colors = {1: 'red', 2: 'yellow'}  # fire=red, smoke=yellow\n",
    "    for ann in entry[\"annotations\"]:\n",
    "        x1, y1, x2, y2 = ann[\"bbox_xyxy\"]\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        class_idx = ann[\"class_idx\"]\n",
    "        class_name = ann[\"class_name\"]\n",
    "        color = colors.get(class_idx, 'blue')\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        ax.text(\n",
    "            x1, y1 - 5, class_name,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),\n",
    "            fontsize=10, color='white' if color == 'red' else 'black'\n",
    "        )\n",
    "\n",
    "# Visualize a few samples from each split\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "idx = 0\n",
    "for split_name in ['train', 'test']:\n",
    "    if split_name in metadata[\"splits\"]:\n",
    "        entries = metadata[\"splits\"][split_name][\"entries\"]\n",
    "        # Get samples with annotations\n",
    "        samples_with_boxes = [e for e in entries if e[\"num_annotations\"] > 0][:3]\n",
    "\n",
    "        for entry in samples_with_boxes:\n",
    "            if idx < len(axes):\n",
    "                visualize_sample(entry, axes[idx])\n",
    "                idx += 1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g4AMThKOOEF-"
   },
   "source": [
    "## 2. Training Progress Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 862
    },
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1765139451785,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "-dvQSdEYOEF-",
    "outputId": "f5d4f63b-8231-4cd1-d81e-27e34208be0b"
   },
   "outputs": [],
   "source": [
    "# Load training history (automatically handles both local and Colab paths)\n",
    "history_path = project_root / \"checkpoints\" / \"training_history.json\"\n",
    "\n",
    "if history_path.exists():\n",
    "    with open(history_path, 'r') as f:\n",
    "        history = json.load(f)\n",
    "\n",
    "    epochs = range(1, len(history[\"train_loss\"]) + 1)\n",
    "\n",
    "    # Plot training curves\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    # Total loss\n",
    "    axes[0, 0].plot(epochs, history[\"train_loss\"], label='Train', marker='o')\n",
    "    axes[0, 0].plot(epochs, history[\"val_loss\"], label='Val', marker='s')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].set_title('Total Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "\n",
    "    # Classification loss\n",
    "    axes[0, 1].plot(epochs, history[\"train_cls\"], label='Train', marker='o')\n",
    "    axes[0, 1].plot(epochs, history[\"val_cls\"], label='Val', marker='s')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Loss')\n",
    "    axes[0, 1].set_title('Classification Loss')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "\n",
    "    # Box regression loss\n",
    "    axes[1, 0].plot(epochs, history[\"train_box\"], label='Train', marker='o')\n",
    "    axes[1, 0].plot(epochs, history[\"val_box\"], label='Val', marker='s')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Loss')\n",
    "    axes[1, 0].set_title('Box Regression Loss')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "\n",
    "    # Centerness loss\n",
    "    axes[1, 1].plot(epochs, history[\"train_ctr\"], label='Train', marker='o')\n",
    "    axes[1, 1].plot(epochs, history[\"val_ctr\"], label='Val', marker='s')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Loss')\n",
    "    axes[1, 1].set_title('Centerness Loss')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Best validation loss: {min(history['val_loss']):.4f} at epoch {history['val_loss'].index(min(history['val_loss'])) + 1}\")\n",
    "else:\n",
    "    print(\"Training history not found. Run training first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5JJonF8mOEF_"
   },
   "source": [
    "## 3. Training Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1765139451787,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "qGJ9aolfOEF_",
    "outputId": "a4ef0351-efd8-44a1-87ae-243a71243ee0"
   },
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "# (Imports already done in cell 1)\n",
    "\n",
    "# Configuration - adjust these as needed\n",
    "# Paths automatically work for both local and Colab\n",
    "config = {\n",
    "    \"metadata_path\": project_root / \"dfire_detection_annotations.json\",\n",
    "    \"batch_size\": 32,\n",
    "    \"num_epochs\": 50,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"target_size\": 800,\n",
    "    \"num_workers\": 4 if not IN_COLAB else 2,  # Colab sometimes has issues with more workers\n",
    "    \"fpn_channels\": 64,\n",
    "    \"stem_channels\": [64, 64],\n",
    "    \"checkpoint_dir\": project_root / \"checkpoints\",\n",
    "    \"log_dir\": project_root / \"logs\",\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "}\n",
    "\n",
    "# Create directories\n",
    "config[\"checkpoint_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "config[\"log_dir\"].mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Setup TensorBoard\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_subdir = config[\"log_dir\"] / f\"run_{timestamp}\"\n",
    "writer = SummaryWriter(log_dir=str(log_subdir))\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "for key, value in config.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "print(f\"\\nTensorBoard logs: {log_subdir}\")\n",
    "print(f\"To view: tensorboard --logdir {config['log_dir']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 350,
     "status": "ok",
     "timestamp": 1765139452133,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "zqIJAYdrOEF_",
    "outputId": "0f33e9b7-6e7f-494f-e8e7-c964e3de3b92"
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "num_classes = 2  # fire and smoke\n",
    "model = FCOS(\n",
    "    num_classes=num_classes,\n",
    "    fpn_channels=config[\"fpn_channels\"],\n",
    "    stem_channels=config[\"stem_channels\"],\n",
    ")\n",
    "model.to(config[\"device\"])\n",
    "\n",
    "print(f\"Model initialized:\")\n",
    "print(f\"  Classes: {num_classes} (0=fire, 1=smoke)\")\n",
    "print(f\"  FPN channels: {config['fpn_channels']}\")\n",
    "print(f\"  Stem channels: {config['stem_channels']}\")\n",
    "print(f\"  Device: {config['device']}\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 165,
     "status": "ok",
     "timestamp": 1765139452296,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "txvE_nK0OEF_",
    "outputId": "2aaa1de0-5bde-49fa-8aa2-50bb42b2572b"
   },
   "outputs": [],
   "source": [
    "# Create dataloaders\n",
    "print(\"Creating dataloaders...\")\n",
    "train_loader, val_loader = create_dataloaders(\n",
    "    config[\"metadata_path\"],\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    target_size=config[\"target_size\"],\n",
    "    num_workers=config[\"num_workers\"],\n",
    "    train_augment=True,\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Train samples: {len(train_loader.dataset)}\")\n",
    "print(f\"Val samples: {len(val_loader.dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1765139452299,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "ZTrLjAZAOEGA",
    "outputId": "d75c3e08-a248-4bfd-b30b-fcf6f6b48874"
   },
   "outputs": [],
   "source": [
    "# Setup optimizer and scheduler\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)\n",
    "\n",
    "print(f\"Optimizer: Adam (lr={config['learning_rate']})\")\n",
    "print(f\"Scheduler: StepLR (step_size=20, gamma=0.1)\")\n",
    "\n",
    "# Training history\n",
    "training_history = {\n",
    "    \"train_loss\": [],\n",
    "    \"val_loss\": [],\n",
    "    \"train_cls\": [],\n",
    "    \"train_box\": [],\n",
    "    \"train_ctr\": [],\n",
    "    \"val_cls\": [],\n",
    "    \"val_box\": [],\n",
    "    \"val_ctr\": [],\n",
    "}\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "start_epoch = 1\n",
    "\n",
    "# Optional: Resume from checkpoint\n",
    "resume_path = None  # Set to project_root / \"checkpoints\" / \"best_model.pth\" to resume\n",
    "\n",
    "if resume_path and resume_path.exists():\n",
    "    print(f\"\\nResuming from checkpoint: {resume_path}\")\n",
    "    checkpoint = torch.load(resume_path, map_location=config[\"device\"])\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    if \"scheduler_state_dict\" in checkpoint:\n",
    "        scheduler.load_state_dict(checkpoint[\"scheduler_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_loss = checkpoint.get(\"val_loss\", float('inf'))\n",
    "    print(f\"Resuming from epoch {start_epoch}, best val loss: {best_val_loss:.4f}\")\n",
    "else:\n",
    "    print(\"\\nStarting fresh training\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcaIu-vMOEGA"
   },
   "source": [
    "## 4. Training Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1765139452606,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "WzZpUWT6OEGA",
    "outputId": "4a97ba14-f72a-4a4b-aaf3-a62ff97427e3"
   },
   "outputs": [],
   "source": [
    "# Training and validation functions\n",
    "def train_epoch(model, dataloader, optimizer, device, epoch, writer=None):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_box_loss = 0.0\n",
    "    total_ctr_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch_idx, (images, gt_boxes) in enumerate(dataloader):\n",
    "        images = images.to(device)\n",
    "        gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        losses = model(images, gt_boxes=gt_boxes)\n",
    "\n",
    "        # Compute total loss\n",
    "        total_loss_batch = (\n",
    "            losses[\"loss_cls\"] +\n",
    "            losses[\"loss_box\"] +\n",
    "            losses[\"loss_ctr\"]\n",
    "        )\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        total_loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate losses\n",
    "        total_loss += total_loss_batch.item()\n",
    "        total_cls_loss += losses[\"loss_cls\"].item()\n",
    "        total_box_loss += losses[\"loss_box\"].item()\n",
    "        total_ctr_loss += losses[\"loss_ctr\"].item()\n",
    "        num_batches += 1\n",
    "\n",
    "        # Log to tensorboard\n",
    "        if writer and batch_idx % 10 == 0:\n",
    "            global_step = epoch * len(dataloader) + batch_idx\n",
    "            writer.add_scalar(\"Train/BatchLoss\", total_loss_batch.item(), global_step)\n",
    "            writer.add_scalar(\"Train/BatchClsLoss\", losses[\"loss_cls\"].item(), global_step)\n",
    "            writer.add_scalar(\"Train/BatchBoxLoss\", losses[\"loss_box\"].item(), global_step)\n",
    "            writer.add_scalar(\"Train/BatchCtrLoss\", losses[\"loss_ctr\"].item(), global_step)\n",
    "\n",
    "        if (batch_idx + 1) % 10 == 0:\n",
    "            print(\n",
    "                f\"  Batch {batch_idx + 1}/{len(dataloader)}, \"\n",
    "                f\"Loss: {total_loss_batch.item():.4f} \"\n",
    "                f\"(cls: {losses['loss_cls'].item():.4f}, \"\n",
    "                f\"box: {losses['loss_box'].item():.4f}, \"\n",
    "                f\"ctr: {losses['loss_ctr'].item():.4f})\"\n",
    "            )\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_cls = total_cls_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_box = total_box_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_ctr = total_ctr_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    return avg_loss, avg_cls, avg_box, avg_ctr\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device, epoch, writer=None):\n",
    "    \"\"\"Validate model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_cls_loss = 0.0\n",
    "    total_box_loss = 0.0\n",
    "    total_ctr_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, gt_boxes in dataloader:\n",
    "            images = images.to(device)\n",
    "            gt_boxes = gt_boxes.to(device)\n",
    "\n",
    "            losses = model(images, gt_boxes=gt_boxes)\n",
    "            total_loss_batch = (\n",
    "                losses[\"loss_cls\"] +\n",
    "                losses[\"loss_box\"] +\n",
    "                losses[\"loss_ctr\"]\n",
    "            )\n",
    "\n",
    "            total_loss += total_loss_batch.item()\n",
    "            total_cls_loss += losses[\"loss_cls\"].item()\n",
    "            total_box_loss += losses[\"loss_box\"].item()\n",
    "            total_ctr_loss += losses[\"loss_ctr\"].item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_cls = total_cls_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_box = total_box_loss / num_batches if num_batches > 0 else 0.0\n",
    "    avg_ctr = total_ctr_loss / num_batches if num_batches > 0 else 0.0\n",
    "\n",
    "    # Log to tensorboard\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Val/Loss\", avg_loss, epoch)\n",
    "        writer.add_scalar(\"Val/ClsLoss\", avg_cls, epoch)\n",
    "        writer.add_scalar(\"Val/BoxLoss\", avg_box, epoch)\n",
    "        writer.add_scalar(\"Val/CtrLoss\", avg_ctr, epoch)\n",
    "\n",
    "    return avg_loss, avg_cls, avg_box, avg_ctr\n",
    "\n",
    "print(\"Training functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MCQ6TapIOEGA"
   },
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6921718,
     "status": "error",
     "timestamp": 1765146374325,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "kSQSvuADOEGA",
    "outputId": "85213e01-629e-4459-d54c-d772f6cf19dd"
   },
   "outputs": [],
   "source": [
    "# Training loop - Run this cell to train all epochs\n",
    "for epoch in range(start_epoch, config[\"num_epochs\"] + 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Epoch {epoch}/{config['num_epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Train\n",
    "    train_loss, train_cls, train_box, train_ctr = train_epoch(\n",
    "        model, train_loader, optimizer, config[\"device\"], epoch, writer\n",
    "    )\n",
    "    print(f\"\\nTrain Loss: {train_loss:.4f} (cls: {train_cls:.4f}, box: {train_box:.4f}, ctr: {train_ctr:.4f})\")\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_cls, val_box, val_ctr = validate(\n",
    "        model, val_loader, config[\"device\"], epoch, writer\n",
    "    )\n",
    "    print(f\"Val Loss: {val_loss:.4f} (cls: {val_cls:.4f}, box: {val_box:.4f}, ctr: {val_ctr:.4f})\")\n",
    "\n",
    "    # Update learning rate\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    writer.add_scalar(\"Train/LearningRate\", current_lr, epoch)\n",
    "    print(f\"Learning rate: {current_lr:.6f}\")\n",
    "\n",
    "    # Save checkpoint\n",
    "    checkpoint = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "        \"train_loss\": train_loss,\n",
    "        \"val_loss\": val_loss,\n",
    "    }\n",
    "\n",
    "    checkpoint_path = config[\"checkpoint_dir\"] / f\"checkpoint_epoch_{epoch}.pth\"\n",
    "    torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_path = config[\"checkpoint_dir\"] / \"best_model.pth\"\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"✓ Saved best model (val_loss: {val_loss:.4f})\")\n",
    "\n",
    "    # Update training history\n",
    "    training_history[\"train_loss\"].append(train_loss)\n",
    "    training_history[\"val_loss\"].append(val_loss)\n",
    "    training_history[\"train_cls\"].append(train_cls)\n",
    "    training_history[\"train_box\"].append(train_box)\n",
    "    training_history[\"train_ctr\"].append(train_ctr)\n",
    "    training_history[\"val_cls\"].append(val_cls)\n",
    "    training_history[\"val_box\"].append(val_box)\n",
    "    training_history[\"val_ctr\"].append(val_ctr)\n",
    "\n",
    "    # Save training history\n",
    "    history_path = config[\"checkpoint_dir\"] / \"training_history.json\"\n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_history, f, indent=2)\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training complete!\")\n",
    "print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
    "print(f\"Checkpoints saved to: {config['checkpoint_dir']}\")\n",
    "print(f\"Logs saved to: {log_subdir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2nHW97hOEGB"
   },
   "source": [
    "## 6. Inference and Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1765146378383,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "4n2sndEnOEGB",
    "outputId": "9e08eb95-a8a5-40e9-d598-ac20bfb9fbae"
   },
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "# Note: If you trained in this notebook, the model is already loaded above\n",
    "# This cell is for loading a previously trained model\n",
    "checkpoint_path = project_root / \"checkpoints\" / \"best_model.pth\"\n",
    "\n",
    "# Option 1: Use the model from training (if you ran training cells above)\n",
    "# The model variable is already available from the training setup\n",
    "\n",
    "# Option 2: Load a previously trained model from checkpoint\n",
    "if 'model' not in globals() or model is None:\n",
    "    if checkpoint_path.exists():\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize model (adjust parameters to match your training)\n",
    "        num_classes = 2  # fire and smoke\n",
    "        model = FCOS(\n",
    "            num_classes=num_classes,\n",
    "            fpn_channels=64,\n",
    "            stem_channels=[64, 64],\n",
    "        )\n",
    "\n",
    "        # Load checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        print(f\"Loaded model from epoch {checkpoint['epoch']}\")\n",
    "        print(f\"Validation loss: {checkpoint['val_loss']:.4f}\")\n",
    "    else:\n",
    "        print(\"Model checkpoint not found. Train a model first.\")\n",
    "        model = None\n",
    "        device = None\n",
    "else:\n",
    "    # Use the model from training\n",
    "    device = config[\"device\"]\n",
    "    model.eval()\n",
    "    print(\"Using model from training setup above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1aRPJ4K3uCUvk_wf65GS-fZVJvNJBrzXZ"
    },
    "executionInfo": {
     "elapsed": 10712,
     "status": "ok",
     "timestamp": 1765146493695,
     "user": {
      "displayName": "Monish Bangalore Vijay Kumar",
      "userId": "08391756839603075774"
     },
     "user_tz": 360
    },
    "id": "PGBMLcDNOEGB",
    "outputId": "f7b8dbbe-b3b6-46fa-ded8-7c01032b4436"
   },
   "outputs": [],
   "source": [
    "def visualize_predictions(image_path, model, device, score_thresh=0.3, nms_thresh=0.5, target_size=800):\n",
    "    \"\"\"Run inference on an image and visualize predictions.\"\"\"\n",
    "    if model is None:\n",
    "        print(\"Model not loaded.\")\n",
    "        return\n",
    "\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    original_size = image.size  # (width, height)\n",
    "\n",
    "    # Resize and normalize\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((target_size, target_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225]\n",
    "        ),\n",
    "    ])\n",
    "\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        pred_boxes, pred_classes, pred_scores = model(\n",
    "            image_tensor,\n",
    "            test_score_thresh=score_thresh,\n",
    "            test_nms_thresh=nms_thresh,\n",
    "        )\n",
    "\n",
    "    # Scale boxes back to original image size\n",
    "    scale_w = original_size[0] / target_size\n",
    "    scale_h = original_size[1] / target_size\n",
    "\n",
    "    pred_boxes = pred_boxes.cpu().numpy()\n",
    "    pred_classes = pred_classes.cpu().numpy()\n",
    "    pred_scores = pred_scores.cpu().numpy()\n",
    "\n",
    "    # Scale boxes\n",
    "    pred_boxes[:, [0, 2]] *= scale_w\n",
    "    pred_boxes[:, [1, 3]] *= scale_h\n",
    "\n",
    "    # Visualize\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    colors = {0: 'red', 1: 'yellow'}  # fire=0 (red), smoke=1 (yellow)\n",
    "    class_names = {0: 'fire', 1: 'smoke'}\n",
    "\n",
    "    for box, cls, score in zip(pred_boxes, pred_classes, pred_scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        color = colors.get(int(cls), 'blue')\n",
    "        class_name = class_names.get(int(cls), f'class_{cls}')\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            (x1, y1), width, height,\n",
    "            linewidth=2, edgecolor=color, facecolor='none'\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "        label_text = f\"{class_name}: {score:.2f}\"\n",
    "        ax.text(\n",
    "            x1, y1 - 5, label_text,\n",
    "            bbox=dict(boxstyle='round,pad=0.3', facecolor=color, alpha=0.7),\n",
    "            fontsize=10, color='white' if color == 'red' else 'black'\n",
    "        )\n",
    "\n",
    "    ax.set_title(f\"Detections: {len(pred_boxes)} objects found\", fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return pred_boxes, pred_classes, pred_scores\n",
    "\n",
    "# Run inference on a few test images\n",
    "if model is not None and device is not None:\n",
    "    test_entries = metadata[\"splits\"][\"test\"][\"entries\"][-15:]\n",
    "\n",
    "    for entry in test_entries:\n",
    "        image_path = Path(entry[\"image_path\"])\n",
    "        print(f\"\\nProcessing: {entry['image_id']}\")\n",
    "        visualize_predictions(image_path, model, device)\n",
    "else:\n",
    "    print(\"Model not loaded. Cannot run inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbSYn8z67E7x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "fire_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
